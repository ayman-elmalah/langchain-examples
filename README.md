# Langchain Examples Documentation

This repository contains examples for using Langchain in various projects. The examples demonstrate how to interact with AI models and integrate them into conversations with simple to advanced setups.

## Table of Contents

1. [Installation](#1-installation)
2. [Directory Structure](#2-directory-structure)
3. [Examples](#3-examples)
    1. [Chat models](#chat-models)
    2. [Basic conversation](#basic-conversation)
    3. [Advanced conversation](#advanced-conversation)
    4. [Basic prompt template](#basic-prompt-template)
    5. [Basic chain](#basic-chain)
    6. [Chain from messages](#chain-from-messages)
    7. [Rag blog](#rag-blog)


## 1. Installation

To get started with the project, follow these steps:

1. Clone the repository:

    ```bash
    git clone https://github.com/ayman-elmalah/langchain-examples
    ```

2. Navigate to the project folder:

    ```bash
    cd langchain-examples
    ```

3. Copy the `.env.example` file to `.env`:

    ```bash
    cp .env.example .env
    ```

4. Add the necessary environment variables in the `.env` file (e.g., API keys for OpenAI, etc.).

5. Install the required dependencies:

    ```bash
    pip install -r requirements.txt
    ```

## 2. Directory Structure

The repository is organized into multiple directories based on the different projects. Below is the structure:

```plaintext
langchain-examples/
├── .env.example
├── .env
├── requirements.txt
├── examples/
│   └── ...
```

## 3. Examples

This repository contains three projects, each demonstrating a different aspect of working with Langchain.

### chat-models

This is a very basic implementation where the user sends a message to the AI model, and the model responds.

- **How it works**: The user sends a single message, and the AI model processes the message and returns a response.
- **To run it**: Simply run the following command:

    ```bash
    python3 examples/chat-models/main.py
    ```

### basic-conversation

This project demonstrates a basic conversation with the AI. It includes a system message and a human message.

- **How it works**: The model is initialized, and a predefined set of messages is sent to the model, including a system message and a human query. The AI responds with an answer.
- **To run it**: To execute the basic conversation example, run the following command:

    ```bash
    python3 examples/basic-conversation/main.py
    ```

### advanced-conversation

This project builds on the previous ones by adding more complex functionality, such as storing and fetching messages from a MySQL database.

- **How it works**: The system interacts with a MySQL database to store and retrieve conversation history. You can send and receive messages, and the AI model will respond based on the previous conversation.
- **Database setup**: You need to create a MySQL database and import the provided SQL schema (`db.sql`).

    1. Create a database in MySQL (e.g., `ai_langchain_test`).
    2. Import the `db.sql` file to set up the necessary tables.
    3. Update your `.env` file with the database connection credentials.

- **To run it**: After setting up the database, run the following command:

    ```bash
    python3 examples/advanced-conversation/main.py 1
    ```

### basic-prompt-template

This project demonstrates a basic prompt template handler to the AI.

- **How it works**: The prompt template is initialized then model invoked, and sent to the model, including messages generated by prmpt template query. The AI responds with an answer.
- **To run it**: To execute the basic prompt template example, run the following command:

    ```bash
    python3 examples/basic-prompt-template/main.py
    ```

### basic-chain

This project demonstrates a basic chain template handler to the AI.

- **How it works**: A prompt template is defined and initialized with placeholders. The template is then connected to the AI model using a chain operation. When the chain is invoked with input values, the prompt is populated with the provided data, sent to the AI model, and the response is generated and returned.
- **To run it**: To execute the basic chain example, run the following command:

    ```bash
    python3 examples/basic-chain/main.py
    ```

### chain-from-messages

This project demonstrates a basic chain using the `from_messages` handler to interact with the AI.

- **How it works**: A chat prompt template is created using multiple predefined messages, including placeholders (e.g., `topic` and `jokes_count`). The template is connected to the AI model through a chain operation. When the chain is invoked with input values, the placeholders are replaced with the provided data, the messages are sent to the AI model, and the output is parsed and returned as a response.
- **To run it**: To execute the `chain-from-messages` example, run the following command:

    ```bash
    python3 examples/chain-from-messages/main.py
    ```

### rag-blog

This project demonstrates how to ingest content from a blog, process it, and use it for retrieval-augmented generation (RAG) with Langchain.

- **How it works**: The `rag-blog` example ingests a blog post from a text file or URL, splits the content into smaller chunks, and stores the processed content in Pinecone using OpenAI embeddings. When a query is provided, the system retrieves relevant content from Pinecone and uses it to generate a response.
- **To run it**: To execute the `rag-blog` example, run the following command:

    1. **Get Pinecone API Key**:
        - Go to [Pinecone](https://www.pinecone.io/) and create a new account.
        - From the sidebar, navigate to **API Keys**.
        - Create a new API key and save it in your `.env` file as `PINECONE_API_KEY`.

    2. **Create Pinecone Index**:
        - From the Pinecone dashboard, go to **Database** → **Indexes**.
        - Click **Create Index** and choose a name and configuration (e.g., **text-embedding-text-small** or another option that fits your needs).
        - Save the index name in your `.env` file as `RAG_BLOG_INDEX_NAME`.

    3. **Run the Ingest Script**:
        Once you've set up your Pinecone API key and index, run the following command to ingest the blog content:


##### Ingest

This will ingest the blog post, split it into chunks, and store the embeddings in Pinecone for future retrieval. You can then use the query functionality to interact with the stored content:

```bash
python3 examples/rag-blog/ingest.py
```

##### Query

This will retrieve relevant information from Pinecone based on the query and generate a response.

```bash
python3 examples/rag-blog/query.py
```
